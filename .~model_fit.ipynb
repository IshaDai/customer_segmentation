{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1_1RnUq-JKyp97sYOuJwyHqQ2ObYIucTt","authorship_tag":"ABX9TyMiyT8Rr55RBWUbMTOo1Jl6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5iwfK-0WjrPJ"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import lightgbm as lgb\n","import matplotlib.pyplot as plt\n","import math \n","\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import confusion_matrix\n","from sklearn import model_selection, metrics\n","from sklearn.model_selection import GridSearchCV, learning_curve, train_test_split\n","from sklearn import neighbors, linear_model, svm, tree, ensemble\n","from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, StackingClassifier\n","from xgboost import XGBClassifier\n","from collections import Counter\n","from datetime import datetime\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","%matplotlib inline"]},{"cell_type":"code","source":["class Model_Fit(object):\n","    def __init__(self, clf, params = None):\n","        if params:\n","            self.clf = clf(**params)\n","        else:\n","            self.clf = clf\n","            \n","    def train(self, x_train, y_train):\n","        self.clf.fit(x_train, y_train)\n","            \n","    def predit(self, x):\n","        return self.clf.predict(x)\n","    \n","    def grid_search(self, parameters, Kfold):\n","        self.grid = GridSearchCV(estimator = self.clf, param_grid = parameters, scoring=\"f1\", n_jobs=-1, verbose=1, cv = Kfold)\n","\n","    def grid_fit(self, x_train, y_train):\n","        self.grid.fit(x_train, y_train)\n","        \n","    def grid_predict(self, x_train, y_train, x_test, y_test):\n","        self.best_model = self.grid.best_estimator_\n","        self.predictions = self.best_model.predict(x_test) # Use the best estimator from GridSearchCV to make predictions\n","        self.y_test_pred = self.best_model.predict(x_test)\n","\n","        train_score = self.best_model.score(x_train, y_train)\n","        test_score = self.best_model.score(x_test, y_test)\n","\n","        print(\"Accuracy score of training dataset2: {:.2f} % \".format(100*train_score))\n","        print(\"Accuracy score of testing dataset2: {:.2f} % \".format(100*test_score))\n","        print(metrics.classification_report(y_test, self.predictions))\n","\n","        return train_score, test_score"],"metadata":{"id":"s0FaaGpEj352"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model algorithms \n","def lgb_model(train_X, train_Y, test_X, test_Y, LGB, k):\n","    print('Lightgbm:')\n","    LGB.grid_search(parameters = [{'learning_rate': [0.05, 0.5, 1], 'max_depth': [-1, -2, -3], 'reg_alpha': [0, 0.1, 1], 'reg_lambda': [0, 0.1, 1], 'n_estimators': [30, 50, 80, 100, 200]}], Kfold = k)\n","    LGB.grid_fit(train_X, train_Y)\n","    train_score, test_score = LGB.grid_predict(train_X, train_Y, test_X, test_Y) \n","\n","    return train_score, test_score\n","    \n","def xgb_model(train_X, train_Y, test_X, test_Y, XGB, k):\n","    print('XGB:')\n","    XGB.grid_search(parameters = {'learning_rate': [0.05, 0.5, 1], 'max_depth': [3,4,5], 'reg_alpha': [0, 0.1, 1], 'reg_lambda': [0, 0.1, 1]} , Kfold = k)\n","    XGB.grid_fit(train_X, train_Y)\n","    train_score, test_score = XGB.grid_predict(train_X, train_Y, test_X, test_Y) \n","\n","    return train_score, test_score\n","\n","def rf_model(train_X, train_Y, test_X, test_Y, RF, k):\n","    print('RF:')\n","    param_grid = {'criterion' : ['entropy', 'gini'], 'n_estimators' : [30, 50, 80, 100, 200], 'max_depth': [3,4,5], 'max_features' :['sqrt', 'log2']}\n","    RF.grid_search(parameters = param_grid, Kfold = k)\n","    RF.grid_fit(train_X, train_Y)\n","    train_score, test_score = RF.grid_predict(train_X, train_Y, test_X, test_Y)\n","\n","    return train_score, test_score\n","\n","def knn_model(train_X, train_Y, test_X, test_Y, KNN, k):\n","    print('KNN:')\n","    KNN.grid_search(parameters = [{'n_neighbors': np.arange(1,50,1), 'leaf_size': [3,6,9,12,15]}], Kfold = k)\n","    KNN.grid_fit(train_X, train_Y)\n","    train_score, test_score = KNN.grid_predict(train_X, train_Y, test_X, test_Y)\n","\n","    return train_score, test_score\n","\n","def vote_model(train_X, train_Y, test_X, test_Y, LGB_best, XGB_best, RF_best, KNN_best, k):\n","    print('Vote:')\n","    votingC = ensemble.VotingClassifier(estimators=[('rf', RF_best),('lgb', LGB_best), ('xgb', XGB_best), ('knn', KNN_best)], voting='soft')\n","    votingC = votingC.fit(train_X, train_Y)\n","    predictions = votingC.predict(test_X)\n","\n","    train_score = votingC.score(train_X, train_Y)\n","    test_score = votingC.score(test_X, test_Y)\n","\n","    print(\"Accuracy score of training dataset: {:.2f} % \".format(100*train_score))\n","    print(\"Accuracy score of testing dataset: {:.2f} % \".format(100*test_score))\n","    print(metrics.classification_report(test_Y, predictions))\n","\n","    return train_score, test_score"],"metadata":{"id":"-TTfIEhyvmgN"},"execution_count":null,"outputs":[]}]}